{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Načítavanie dát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data_phase_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podiel práce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tulach 50%\n",
    "\n",
    "Černega 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Riešenie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na konci prvej fázy zadania sme si exportli mergnutý dataset všetkých 4 tabuliek a ten sme následne importovali sem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moje zdôvodnenie pre tento krok je ten, že sme v prvej fáze niektoré stĺpce niektorých tabuliek mazali a prerábali na dva osobitné stĺpce (konkrétne stĺpec location sme úplne zmazali a tie dáta sme prerobili na latitude a longtitude, keďže je lepšie mať univerzálny formát dát). Sekvenčne je vytvorenie testovacej a trénovacej sady z týchto dát ako prvá úloha v tejto fáze zadania, a síce už máme odstránených outlierov, mohli nám vzniknúť nové, ktoré by sme dokázali odstrániť pomocou Pipeline tak či tak. Vytvoríme si teda testovaciu a trénovaciu sadu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable and features\n",
    "X = data.drop(columns=['mwra'])\n",
    "y = data['mwra']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ďalej budeme už robiť zatiaľ s trénovacím datasetom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najprv vytvoríme nový dataframe, kde odstránime riadky s nedefinovanými hodnotami z nášho daného datasetu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NA values from the training dataset\n",
    "X_train_cleaned = X_train.dropna()\n",
    "y_train_cleaned = y_train[X_train_cleaned.index]\n",
    "\n",
    "# Print the number of rows in the cleaned training dataset\n",
    "print(f\"Number of rows in the cleaned training dataset: {X_train_cleaned.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme, že z pôvodných 36468 hodnôt sa nám počet zredukoval na 7779, preto je to celkom nešťastné riešenie a bolo by lepšie, keby sme chýbajúce hodnoty nejakou štatistickou metódou do nášho datasetu doplnili.\n",
    "\n",
    "Tu teda doplníme hodnoty tak, že pre každý atribút sa zoberie priemerná hodnota, resp. najčastejšia hodnota pre kategorické atribúty a doplní sa do jednotlivých atribútov pomocou triedy SimpleImputer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create imputers\n",
    "imputer_num = SimpleImputer(strategy='mean')\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train[numerical_cols] = imputer_num.fit_transform(X_train[numerical_cols])\n",
    "X_train[categorical_cols] = imputer_cat.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "# Check if every column has the same amount of non-null values\n",
    "non_null_counts = X_train.notnull().sum()\n",
    "print(non_null_counts)\n",
    "all_equal = non_null_counts.nunique() == 1\n",
    "print(f\"All columns have the same amount of non-null values: {all_equal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme, že každý stĺpec má rovnaký počet hodnôt, ktoré sú nenulové, to znamená, že sa úspešne nahradili nulové hodnoty v datasete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[categorical_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu môžeme vidieť stĺpce, ktoré obsahujú kategorické hodnoty, ktoré môžeme transformovať na numerické dáta na trénovanie modelu, avšak niektoré dáta ako ssn (social security number) alebo username môžu byť unikátne, tak by nemali veľkú výpovednú hodnotu pre trénovanie modelu. Atribúty, čo budú mať určite väčšiu výpovednú hodnotu sú job a location. Môžeme použiť ordinal encoding na všetky tieto stĺpce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Initialize the OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "X_train[categorical_cols] = ordinal_encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "# Display the first few rows of the transformed dataframe\n",
    "print(X_train.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
